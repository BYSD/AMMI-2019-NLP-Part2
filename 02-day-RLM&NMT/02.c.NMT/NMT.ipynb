{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup> with inputs from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html </sup>\n",
    "\n",
    "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash pyfiles/download_reqs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Google Translate API for Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ssut/py-googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_utils = 'pyfiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(path_to_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import global_variables\n",
    "import nmt_dataset\n",
    "import nnet_models_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_saved_models_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a English to French Dataset from https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = 'en'\n",
    "target_name = 'fr'\n",
    "\n",
    "path_to_train_data = {'source':main_data_path+'train.'+source_name, \n",
    "                      'target':main_data_path+'train.'+target_name}\n",
    "path_to_val_data = {'source': main_data_path+'valid.'+source_name, \n",
    "                      'target':main_data_path+'valid.'+target_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See first 5 records\n",
    "\n",
    "! head -5 'data/train.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and making PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make it a pair - (source, target) sentence pair. For this, we have to read the file and parse it accordingly. We might have to take care of some details there, like making sure that we strip off any non-required special characters or extra space. All those boring details aside (which you can see in dataset_helper.py) what are the other things we have to do?\n",
    "\n",
    "We have to make a vocabulary and tokenize like we have been doing. Here, we are writing a Language Class, like we did in the previous labs to take care of this for you. Once we have done all this and tokenized, we write a pytorch dataset object to help as handle this efficiently during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {'train': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_train_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                     minimum_count = 1), \n",
    "\n",
    "                'val': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_val_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                    minimum_count = 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LanguagePair object we built has a DataFrame underneath. We see the first 5 rows of the dataframe below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'].main_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary sizes and sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vocabulary sizes\n",
    "print('source vocab: ', dataset_dict['train'].source_lang_obj.n_words , \n",
    "      'target vocab: ', dataset_dict['train'].target_lang_obj.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vocabulary sizes\n",
    "print('max len: ', dataset_dict['train'].main_df['source_len'].max(), \n",
    "      'min len: ', dataset_dict['train'].main_df['source_len'].min() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'].main_df['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51 looks like a very long sentence and at the $99.99$th percentile is 32. We probably don't want that much. How do we get rid of rest of the words or clip sentence at some MAX LEN? We can use the collate function of pytorch that we had seen earlier to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.9999))\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0), \n",
    "                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of usually of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"I am not the\n",
    "black cat\" → \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
    "\n",
    "Usually, the encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will first start with a BoW encoder and then move on to RNN based encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "source_lang_obj = dataset_dict['train'].source_lang_obj\n",
    "target_lang_obj = dataset_dict['train'].target_lang_obj\n",
    "\n",
    "source_vocab = dataset_dict['train'].source_lang_obj.n_words;\n",
    "target_vocab = dataset_dict['train'].target_lang_obj.n_words;\n",
    "hidden_size = 512\n",
    "rnn_layers = 1\n",
    "lr = 0.25;\n",
    "longest_label = 1;\n",
    "gradient_clip = 0.3;\n",
    "use_cuda = True\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagOfWords Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_bow = nnet_models_new.BagOfWords(input_size = source_vocab,\n",
    "                                    hidden_size = hidden_size, \n",
    "                                    nlayers=4, \n",
    "                                    reduce = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "--------------------\n",
    "\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "Decoder w/o Attention\n",
    "------------------------\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_bow = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_bow = nnet_models_new.seq2seq(encoder_bow, decoder_bow,\n",
    "                              lr = 1e-2, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_filepath(path, enc_type):\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    return os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(nmt_model, path, enc_type):\n",
    "    if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    torch.save(nmt_model, os.path.join(path, filename))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, nmt, num_epochs=50, val_every=1, saved_model_path = '.', enc_type ='rnn'):\n",
    "\n",
    "    best_bleu = -1;\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, num_epochs));\n",
    "        \n",
    "        for i, data in tqdm(enumerate(dataloader['train']), total=len(dataloader['train'])):  \n",
    "            _, curr_loss = nmt.train_step(data);\n",
    "            running_loss += curr_loss\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader['train']) \n",
    "        \n",
    "        print(\"epoch {} loss = {}, time = {}\".format(epoch, epoch_loss,\n",
    "                                                        time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "   \n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score = nmt.get_bleu_score(dataloader['val']);\n",
    "            print('validation bleu: ', val_bleu_score)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            nmt.scheduler_step(val_bleu_score);\n",
    "            \n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                save_models(nmt, saved_model_path, enc_type);\n",
    "\n",
    "        print('='*50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Bow Encoder GRU Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = False\n",
    "modelname = 'bow'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
    "    nmt_bow = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    train_model(dataloader_dict, nmt_bow, \n",
    "                          num_epochs = num_epochs, \n",
    "                          saved_model_path = saved_models_dir, \n",
    "                          enc_type = 'bow_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmt_bow.get_bleu_score(dataloader_dict['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def get_binned_bl_score(nmt_model, val_dataset):\n",
    "    \n",
    "#     source_len = np.sort(np.array(val_dataset.main_df['source_len']))\n",
    "#     len_threshold = [0]+[source_len[x*1000+376] for x in range(1, 21)];\n",
    "#     len_threshold = np.unique(len_threshold)\n",
    "    len_threshold = np.arange(0, 31, 5)\n",
    "    bin_bl_score = np.zeros(len(len_threshold));\n",
    "    \n",
    "    for i in tqdm( range(1, len(len_threshold)), total = len(len_threshold)-1):\n",
    "        min_len = len_threshold[i-1]\n",
    "#         min_len = 0\n",
    "        max_len = len_threshold[i]\n",
    "        \n",
    "        temp_dataset = copy.deepcopy(val_dataset);\n",
    "        temp_dataset.main_df = temp_dataset.main_df[(temp_dataset.main_df['source_len'] > min_len) & (temp_dataset.main_df['source_len'] <= max_len)];\n",
    "        temp_loader = DataLoader(temp_dataset, batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=100),\n",
    "                            shuffle = True, num_workers=0)\n",
    "        \n",
    "        bin_bl_score[i] = nmt_model.get_bleu_score(temp_loader);\n",
    "        \n",
    "    \n",
    "    len_threshold = len_threshold[1:]\n",
    "    bin_bl_score = bin_bl_score[1:]\n",
    "    \n",
    "    plt.plot(len_threshold, bin_bl_score, 'x-')\n",
    "    plt.ylim(0, np.max(bin_bl_score)+1)\n",
    "    plt.xlabel('len')\n",
    "    plt.ylabel('bl score')\n",
    "    \n",
    "    return len_threshold, bin_bl_score\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       [global_variables.EOS_TOKEN], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ')+\n",
    "                       [global_variables.EOS_TOKEN]);\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_encoded_batch(sentence, lang_obj, use_cuda):\n",
    "    \"\"\" accepts only bsz = 1.\n",
    "        input: one sentence as a string\n",
    "        output: named tuple with vector and length\"\"\"\n",
    "    \n",
    "    sentence = sentence + ' ' + global_variables.EOS_TOKEN;\n",
    "    tensor = source_lang_obj.txt2vec(sentence).unsqueeze(0)\n",
    "    \n",
    "    device = torch.device('cuda') if use_cuda and torch.cuda.is_available() else torch.device('cpu');\n",
    "    \n",
    "    named_returntuple = namedtuple('namedtuple', ['text_vecs', 'text_lens', 'label_vecs', 'label_lens', 'use_packed'])\n",
    "    return_tuple = named_returntuple( tensor.to(device), \n",
    "                                     torch.from_numpy(np.array([tensor.shape[-1]])).to(device),\n",
    "                                     None,\n",
    "                                     None,\n",
    "                                     False );\n",
    "\n",
    "    return return_tuple\n",
    "\n",
    "def get_translation(nmt_model, sentence, lang_obj, use_cuda):\n",
    "    print('souce: ', sentence)\n",
    "    batch = get_encoded_batch(sentence, lang_obj, use_cuda);\n",
    "    prediction, attn_scores_list = nmt_model.eval_step(batch, return_attn = True);\n",
    "    prediction = prediction[0];\n",
    "    print('prediction: ', prediction)\n",
    "    print('GT on sentence (src->tgt): ', translator.translate(sentence, \n",
    "                                                     src = source_name,\n",
    "                                                     dest = target_name).text)\n",
    "    print('GT on prediction (tgt->src): ', translator.translate(prediction, \n",
    "                                                     src = target_name,\n",
    "                                                     dest = source_name).text)\n",
    "\n",
    "    if attn_scores_list[0] is not None:\n",
    "        if attn_scores_list[0][0] is not None:\n",
    "            attn_matrix = [x[0].data.cpu().numpy() for x in attn_scores_list];\n",
    "            attn_matrix = np.stack(attn_matrix)[:,:, 0]\n",
    "            showAttention(sentence, prediction, attn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_bow, 'hello how are you ?', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_bow, 'are hello ? how you', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_rnn = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_rnn = nnet_models_new.seq2seq(encoder_rnn, decoder_rnn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = False\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, 'rnn')) and (not train_again):\n",
    "    nmt_rnn = torch.load(get_full_filepath(saved_models_dir, 'rnn'))\n",
    "else:\n",
    "    train_model(dataloader_dict, nmt_rnn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'rnn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmt_rnn.get_bleu_score(dataloader_dict['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_rnn, 'hello how are you ?', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_rnn, 'are hello ? how you', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_rnn, 'i know that the last thing you want to do is help me .', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_len_threshold, rnn_bin_bl = get_binned_bl_score(nmt_rnn, dataset_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + Source Side Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention = True\n",
    "self_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_encoderattn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_encoderattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n",
    "                                 hidden_size=hidden_size, \n",
    "                                 encoder_attention = encoder_attention,\n",
    "                                 self_attention = self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_encoderattn = nnet_models_new.seq2seq(encoder_encoderattn, decoder_encoderattn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = False\n",
    "modelname = 'encoderattn'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
    "    nmt_encoderattn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    train_model(dataloader_dict, nmt_encoderattn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'encoderattn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmt_encoderattn.get_bleu_score(dataloader_dict['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU vs Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_len_threshold, attn_bin_bl = get_binned_bl_score(nmt_encoderattn, dataset_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn_len_threshold, rnn_bin_bl, '--x', label = 'w/o attn')\n",
    "plt.plot(attn_len_threshold, attn_bin_bl, '--x', label = 'attn')\n",
    "plt.xlabel('len sentence')\n",
    "plt.ylabel('bl score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_encoderattn, 'hello how are you ?', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_encoderattn, 'she knows better than to argue with him .', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_encoderattn, 'she s five years years than me .', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(nmt_encoderattn, 'i know that the last thing you want to do is help me .', source_lang_obj, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Encoder, Self Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention = False\n",
    "self_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_selfattn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_selfattn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_selfattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n",
    "                                 hidden_size=hidden_size, \n",
    "                                 encoder_attention = encoder_attention,\n",
    "                                 self_attention = self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_selfattn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_selfattn = nnet_models_new.seq2seq(encoder_selfattn, decoder_selfattn,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, \n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_again = False\n",
    "modelname = 'selfattn'\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
    "    nmt_selfattn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
    "else:\n",
    "    train_model(dataloader_dict, nmt_selfattn, \n",
    "                      num_epochs = 20, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'selfattn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_selfattn.get_bleu_score(dataloader_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
